{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed93293c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78be8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sent_train = pd.read_csv(\"Data/training.1600000.processed.noemoticon.csv\").iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc3a7166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>QUERY</th>\n",
       "      <th>Username</th>\n",
       "      <th>OriginalTweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1048574</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186607</td>\n",
       "      <td>Fri May 29 07:33:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sugababez</td>\n",
       "      <td>cant wait til her date this weekend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048573</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186445</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>jonasobsessedx</td>\n",
       "      <td>@DestinyHope92 im great thaanks  wbuu?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048572</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186429</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Falchion</td>\n",
       "      <td>@ShaDeLa same here  say it like from the Termi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048571</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186409</td>\n",
       "      <td>Fri May 29 07:33:43 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>OffRoad_Dude</td>\n",
       "      <td>Mid-morning snack time... A bowl of cheese noo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048570</th>\n",
       "      <td>4</td>\n",
       "      <td>1960186342</td>\n",
       "      <td>Fri May 29 07:33:44 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Madelinedugganx</td>\n",
       "      <td>My GrandMa is making Dinenr with my Mum</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sentiment          ID                          Date     QUERY  \\\n",
       "1048574          4  1960186607  Fri May 29 07:33:45 PDT 2009  NO_QUERY   \n",
       "1048573          4  1960186445  Fri May 29 07:33:44 PDT 2009  NO_QUERY   \n",
       "1048572          4  1960186429  Fri May 29 07:33:44 PDT 2009  NO_QUERY   \n",
       "1048571          4  1960186409  Fri May 29 07:33:43 PDT 2009  NO_QUERY   \n",
       "1048570          4  1960186342  Fri May 29 07:33:44 PDT 2009  NO_QUERY   \n",
       "\n",
       "                Username                                      OriginalTweet  \n",
       "1048574        sugababez               cant wait til her date this weekend   \n",
       "1048573   jonasobsessedx             @DestinyHope92 im great thaanks  wbuu?  \n",
       "1048572         Falchion  @ShaDeLa same here  say it like from the Termi...  \n",
       "1048571     OffRoad_Dude  Mid-morning snack time... A bowl of cheese noo...  \n",
       "1048570  Madelinedugganx           My GrandMa is making Dinenr with my Mum   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sent_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bcb84ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@kenichan', 'dived', 'times', 'ball', 'managed', 'save', '50', ' ', 'rest', 'bounds']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# Create our list of punctuation marks\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# Create our list of stopwords\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "parser = English()\n",
    "\n",
    "# Creating our tokenizer function\n",
    "def spacy_tokenizer(sentence):\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = parser(sentence)\n",
    "    # Lemmatizing each token and converting each token into lowercase\n",
    "    mytokens = [ word.lower_ for word in mytokens ]\n",
    "    # Removing stop words\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    # return preprocessed list of tokens\n",
    "    return mytokens\n",
    "print(spacy_tokenizer(df_sent_train['OriginalTweet'][1]))\n",
    "\n",
    "# Custom transformer using spaCy\n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c6f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "248576\n",
      "799999\n",
      "475401\n",
      "475401\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "OT = df_sent_train['OriginalTweet'] # the features we want to analyze\n",
    "pre_sent = df_sent_train['Sentiment'] # the labels, or answers, we want to test against\n",
    "\n",
    "Sent = []\n",
    "neg = 0\n",
    "pos = 0\n",
    "for rank in pre_sent:    \n",
    "    if rank == 0:\n",
    "        neg = neg +1\n",
    "        if(neg <= pos*1.25):\n",
    "            Sent.append(0)\n",
    "    else: \n",
    "        Sent.append(1)\n",
    "        pos = pos +1\n",
    "\n",
    "print(pos)\n",
    "print(neg)\n",
    "c = list(zip(OT,Sent))\n",
    "random.shuffle(c)\n",
    "OT, Sent = zip(*c)\n",
    "    \n",
    "OT_train, OT_val, Sent_train, Sent_val = train_test_split(OT, Sent, test_size=0.15)\n",
    "print(len(OT_train))\n",
    "print(len(Sent_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b60f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision, Recall and F1 are all based on if the tweet are classified \n",
    "#as \"Not Negative\" in any way\n",
    "from sklearn import metrics\n",
    "def report(method, predictions, truth):\n",
    "    print(method)\n",
    "    print(\"Accuracy:\",metrics.accuracy_score(truth, predictions))\n",
    "    print(\"Precision:\",metrics.precision_score(truth, predictions))\n",
    "    print(\"Recall:\",metrics.recall_score(truth, predictions))\n",
    "    F1 = 2*((metrics.precision_score(truth, predictions)*metrics.recall_score(truth, predictions))\n",
    "            /(metrics.precision_score(truth, predictions)+metrics.recall_score(truth, predictions)))\n",
    "    print(\"F1: \", F1)\n",
    "    return [method, metrics.accuracy_score(truth, predictions), metrics.precision_score(truth, predictions), \n",
    "            metrics.recall_score(truth, predictions), F1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881731b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier\n",
      "Accuracy: 0.7613683771380892\n",
      "Precision: 0.7670815183571873\n",
      "Recall: 0.6629557921910294\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(bootstrap = False, \n",
    "                            criterion = 'entropy',\n",
    "                            n_estimators = 100, )\n",
    "forestpipe = Pipeline([('bow',CountVectorizer()),\n",
    "                     ('clf',clf)])\n",
    "forestpipe.fit(OT_train[:20000],Sent_train[:20000])\n",
    "predicted = forestpipe.predict(OT_val)\n",
    "report(\"Random Forest Classifier\", predicted, Sent_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59887af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = pd.read_csv(\"Data/Transcripts/MFs1CSV.csv\")\n",
    "s2 = pd.read_csv(\"Data/Transcripts/MFs2CSV.csv\")\n",
    "s3 = pd.read_csv(\"Data/Transcripts/MFs3CSV.csv\")\n",
    "s5 = pd.read_csv(\"Data/Transcripts/MFs5CSV.csv\")\n",
    "seasons = [s1, s2, s3, s5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92e1d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "phil = []\n",
    "claire = []\n",
    "haley = []\n",
    "alex = []\n",
    "luke = []\n",
    "gloria = []\n",
    "jay = []\n",
    "manny = []\n",
    "cam = []\n",
    "mitch = []\n",
    "lily = []\n",
    "chars = [phil, claire, haley, alex, luke, gloria, jay, manny, cam, mitch, lily]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4770e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns an array structured such as \n",
    "#array[Character][Season]\n",
    "\n",
    "def Char_Sent(Characters, Data):\n",
    "    row = 0\n",
    "    full_p = []\n",
    "    for char in Characters:\n",
    "        predictions = []\n",
    "        for season in Data:\n",
    "            predict = forestpipe.predict(season.iloc[row].dropna())\n",
    "            predictions.append(predict)\n",
    "        full_p.append(predictions)\n",
    "        row = row+1\n",
    "    return full_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb15b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sentiments = Char_Sent(chars, seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9d3ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns an array structued as \n",
    "#array[Season][Character]\n",
    "def Season_Sent(Data):\n",
    "    seasonal = []\n",
    "    for season in Data:\n",
    "        predictions = []\n",
    "        for x in range(11):\n",
    "            predict = forestpipe.predict(season.iloc[x][1:].dropna())\n",
    "            predictions.append(predict)\n",
    "        seasonal.append(predictions)\n",
    "    return seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d10cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_sentiments = Season_Sent(seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3d0c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229d0375",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c10aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c3253a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c30ead1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
